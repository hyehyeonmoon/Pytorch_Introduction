{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "nn.Module_logistic.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNSZOMHXC5PR5vrAN+LHNp9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/angelfox4/Pytorch_Introduction/blob/main/nn_Module_logistic.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P45oXMWKmOIU"
      },
      "source": [
        "# nn.Module로 구현하는 로지스틱 회귀"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5BU0Ao7SmRP0"
      },
      "source": [
        "## 파이토치의 nn.Linear와 nn.Sigmoid로 로지스틱 회귀 구현하기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mfOKue18mRSO"
      },
      "source": [
        "import torch\r\n",
        "import torch.nn as nn\r\n",
        "import torch.nn.functional as F\r\n",
        "import torch.optim as optim"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Flih7AMmRU3",
        "outputId": "8e1a08fc-ebf4-4db2-9822-dc611c14b225"
      },
      "source": [
        "torch.manual_seed(1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7efd0c7b7b58>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6m_mHQwemRXT"
      },
      "source": [
        "x_data = [[1, 2], [2, 3], [3, 1], [4, 3], [5, 3], [6, 2]]\r\n",
        "y_data = [[0], [0], [0], [1], [1], [1]]\r\n",
        "x_train = torch.FloatTensor(x_data)\r\n",
        "y_train = torch.FloatTensor(y_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HexTZcr8mRZq"
      },
      "source": [
        "#nn.Sequential()은 nn.Module 층을 차례로 쌓을 수 있도록 함\r\n",
        "model = nn.Sequential(\r\n",
        "   nn.Linear(2, 1), # input_dim = 2, output_dim = 1\r\n",
        "   nn.Sigmoid() # 출력은 시그모이드 함수를 거친다\r\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RiYBBaKRmglP",
        "outputId": "b7643008-f018-4743-9d53-9f0fd0ed9a85"
      },
      "source": [
        "model(x_train) #현재 W와 b는 랜덤 초기화 된 상태"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.4020],\n",
              "        [0.4147],\n",
              "        [0.6556],\n",
              "        [0.5948],\n",
              "        [0.6788],\n",
              "        [0.8061]], grad_fn=<SigmoidBackward>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qy4S1ZTUmgoA",
        "outputId": "a2eabfa2-30c4-4f1d-cd06-04a5efc22aa4"
      },
      "source": [
        "# optimizer 설정\r\n",
        "optimizer = optim.SGD(model.parameters(), lr=1)\r\n",
        "\r\n",
        "nb_epochs = 1000\r\n",
        "for epoch in range(nb_epochs + 1):\r\n",
        "\r\n",
        "    # H(x) 계산\r\n",
        "    hypothesis = model(x_train)\r\n",
        "\r\n",
        "    # cost 계산\r\n",
        "    cost = F.binary_cross_entropy(hypothesis, y_train)\r\n",
        "\r\n",
        "    # cost로 H(x) 개선\r\n",
        "    optimizer.zero_grad()\r\n",
        "    cost.backward()\r\n",
        "    optimizer.step()\r\n",
        "\r\n",
        "    # 20번마다 로그 출력\r\n",
        "    if epoch % 10 == 0:\r\n",
        "        prediction = hypothesis >= torch.FloatTensor([0.5]) # 예측값이 0.5를 넘으면 True로 간주\r\n",
        "        correct_prediction = prediction.float() == y_train # 실제값과 일치하는 경우만 True로 간주\r\n",
        "        accuracy = correct_prediction.sum().item() / len(correct_prediction) # 정확도를 계산\r\n",
        "        print('Epoch {:4d}/{} Cost: {:.6f} Accuracy {:2.2f}%'.format( # 각 에포크마다 정확도를 출력\r\n",
        "            epoch, nb_epochs, cost.item(), accuracy * 100,\r\n",
        "        ))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch    0/1000 Cost: 0.539713 Accuracy 83.33%\n",
            "Epoch   10/1000 Cost: 0.614853 Accuracy 66.67%\n",
            "Epoch   20/1000 Cost: 0.441875 Accuracy 66.67%\n",
            "Epoch   30/1000 Cost: 0.373145 Accuracy 83.33%\n",
            "Epoch   40/1000 Cost: 0.316358 Accuracy 83.33%\n",
            "Epoch   50/1000 Cost: 0.266094 Accuracy 83.33%\n",
            "Epoch   60/1000 Cost: 0.220498 Accuracy 100.00%\n",
            "Epoch   70/1000 Cost: 0.182095 Accuracy 100.00%\n",
            "Epoch   80/1000 Cost: 0.157299 Accuracy 100.00%\n",
            "Epoch   90/1000 Cost: 0.144091 Accuracy 100.00%\n",
            "Epoch  100/1000 Cost: 0.134272 Accuracy 100.00%\n",
            "Epoch  110/1000 Cost: 0.125769 Accuracy 100.00%\n",
            "Epoch  120/1000 Cost: 0.118297 Accuracy 100.00%\n",
            "Epoch  130/1000 Cost: 0.111680 Accuracy 100.00%\n",
            "Epoch  140/1000 Cost: 0.105779 Accuracy 100.00%\n",
            "Epoch  150/1000 Cost: 0.100483 Accuracy 100.00%\n",
            "Epoch  160/1000 Cost: 0.095704 Accuracy 100.00%\n",
            "Epoch  170/1000 Cost: 0.091369 Accuracy 100.00%\n",
            "Epoch  180/1000 Cost: 0.087420 Accuracy 100.00%\n",
            "Epoch  190/1000 Cost: 0.083806 Accuracy 100.00%\n",
            "Epoch  200/1000 Cost: 0.080486 Accuracy 100.00%\n",
            "Epoch  210/1000 Cost: 0.077425 Accuracy 100.00%\n",
            "Epoch  220/1000 Cost: 0.074595 Accuracy 100.00%\n",
            "Epoch  230/1000 Cost: 0.071969 Accuracy 100.00%\n",
            "Epoch  240/1000 Cost: 0.069526 Accuracy 100.00%\n",
            "Epoch  250/1000 Cost: 0.067248 Accuracy 100.00%\n",
            "Epoch  260/1000 Cost: 0.065118 Accuracy 100.00%\n",
            "Epoch  270/1000 Cost: 0.063122 Accuracy 100.00%\n",
            "Epoch  280/1000 Cost: 0.061247 Accuracy 100.00%\n",
            "Epoch  290/1000 Cost: 0.059483 Accuracy 100.00%\n",
            "Epoch  300/1000 Cost: 0.057820 Accuracy 100.00%\n",
            "Epoch  310/1000 Cost: 0.056250 Accuracy 100.00%\n",
            "Epoch  320/1000 Cost: 0.054764 Accuracy 100.00%\n",
            "Epoch  330/1000 Cost: 0.053357 Accuracy 100.00%\n",
            "Epoch  340/1000 Cost: 0.052022 Accuracy 100.00%\n",
            "Epoch  350/1000 Cost: 0.050753 Accuracy 100.00%\n",
            "Epoch  360/1000 Cost: 0.049546 Accuracy 100.00%\n",
            "Epoch  370/1000 Cost: 0.048396 Accuracy 100.00%\n",
            "Epoch  380/1000 Cost: 0.047299 Accuracy 100.00%\n",
            "Epoch  390/1000 Cost: 0.046252 Accuracy 100.00%\n",
            "Epoch  400/1000 Cost: 0.045251 Accuracy 100.00%\n",
            "Epoch  410/1000 Cost: 0.044294 Accuracy 100.00%\n",
            "Epoch  420/1000 Cost: 0.043376 Accuracy 100.00%\n",
            "Epoch  430/1000 Cost: 0.042497 Accuracy 100.00%\n",
            "Epoch  440/1000 Cost: 0.041653 Accuracy 100.00%\n",
            "Epoch  450/1000 Cost: 0.040843 Accuracy 100.00%\n",
            "Epoch  460/1000 Cost: 0.040064 Accuracy 100.00%\n",
            "Epoch  470/1000 Cost: 0.039315 Accuracy 100.00%\n",
            "Epoch  480/1000 Cost: 0.038593 Accuracy 100.00%\n",
            "Epoch  490/1000 Cost: 0.037898 Accuracy 100.00%\n",
            "Epoch  500/1000 Cost: 0.037228 Accuracy 100.00%\n",
            "Epoch  510/1000 Cost: 0.036582 Accuracy 100.00%\n",
            "Epoch  520/1000 Cost: 0.035958 Accuracy 100.00%\n",
            "Epoch  530/1000 Cost: 0.035356 Accuracy 100.00%\n",
            "Epoch  540/1000 Cost: 0.034773 Accuracy 100.00%\n",
            "Epoch  550/1000 Cost: 0.034210 Accuracy 100.00%\n",
            "Epoch  560/1000 Cost: 0.033664 Accuracy 100.00%\n",
            "Epoch  570/1000 Cost: 0.033137 Accuracy 100.00%\n",
            "Epoch  580/1000 Cost: 0.032625 Accuracy 100.00%\n",
            "Epoch  590/1000 Cost: 0.032130 Accuracy 100.00%\n",
            "Epoch  600/1000 Cost: 0.031649 Accuracy 100.00%\n",
            "Epoch  610/1000 Cost: 0.031183 Accuracy 100.00%\n",
            "Epoch  620/1000 Cost: 0.030730 Accuracy 100.00%\n",
            "Epoch  630/1000 Cost: 0.030291 Accuracy 100.00%\n",
            "Epoch  640/1000 Cost: 0.029864 Accuracy 100.00%\n",
            "Epoch  650/1000 Cost: 0.029449 Accuracy 100.00%\n",
            "Epoch  660/1000 Cost: 0.029046 Accuracy 100.00%\n",
            "Epoch  670/1000 Cost: 0.028654 Accuracy 100.00%\n",
            "Epoch  680/1000 Cost: 0.028272 Accuracy 100.00%\n",
            "Epoch  690/1000 Cost: 0.027900 Accuracy 100.00%\n",
            "Epoch  700/1000 Cost: 0.027538 Accuracy 100.00%\n",
            "Epoch  710/1000 Cost: 0.027186 Accuracy 100.00%\n",
            "Epoch  720/1000 Cost: 0.026842 Accuracy 100.00%\n",
            "Epoch  730/1000 Cost: 0.026507 Accuracy 100.00%\n",
            "Epoch  740/1000 Cost: 0.026181 Accuracy 100.00%\n",
            "Epoch  750/1000 Cost: 0.025862 Accuracy 100.00%\n",
            "Epoch  760/1000 Cost: 0.025552 Accuracy 100.00%\n",
            "Epoch  770/1000 Cost: 0.025248 Accuracy 100.00%\n",
            "Epoch  780/1000 Cost: 0.024952 Accuracy 100.00%\n",
            "Epoch  790/1000 Cost: 0.024663 Accuracy 100.00%\n",
            "Epoch  800/1000 Cost: 0.024381 Accuracy 100.00%\n",
            "Epoch  810/1000 Cost: 0.024104 Accuracy 100.00%\n",
            "Epoch  820/1000 Cost: 0.023835 Accuracy 100.00%\n",
            "Epoch  830/1000 Cost: 0.023571 Accuracy 100.00%\n",
            "Epoch  840/1000 Cost: 0.023313 Accuracy 100.00%\n",
            "Epoch  850/1000 Cost: 0.023061 Accuracy 100.00%\n",
            "Epoch  860/1000 Cost: 0.022814 Accuracy 100.00%\n",
            "Epoch  870/1000 Cost: 0.022572 Accuracy 100.00%\n",
            "Epoch  880/1000 Cost: 0.022336 Accuracy 100.00%\n",
            "Epoch  890/1000 Cost: 0.022104 Accuracy 100.00%\n",
            "Epoch  900/1000 Cost: 0.021877 Accuracy 100.00%\n",
            "Epoch  910/1000 Cost: 0.021655 Accuracy 100.00%\n",
            "Epoch  920/1000 Cost: 0.021437 Accuracy 100.00%\n",
            "Epoch  930/1000 Cost: 0.021224 Accuracy 100.00%\n",
            "Epoch  940/1000 Cost: 0.021015 Accuracy 100.00%\n",
            "Epoch  950/1000 Cost: 0.020810 Accuracy 100.00%\n",
            "Epoch  960/1000 Cost: 0.020609 Accuracy 100.00%\n",
            "Epoch  970/1000 Cost: 0.020412 Accuracy 100.00%\n",
            "Epoch  980/1000 Cost: 0.020219 Accuracy 100.00%\n",
            "Epoch  990/1000 Cost: 0.020029 Accuracy 100.00%\n",
            "Epoch 1000/1000 Cost: 0.019843 Accuracy 100.00%\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}